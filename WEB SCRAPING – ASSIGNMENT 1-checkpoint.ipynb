{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Write a python program to display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function which displays all header tags from a web page, given the url of a webpage as parameter\n",
    "def all_header_tags(url) :\n",
    "    page = requests.get(url) # request data from web page\n",
    "    soup = BeautifulSoup(page.content,'html.parser') # parsing \n",
    "    h_tags = soup.find_all(['h1','h2','h3','h4','h5','h6']) # find all header tags\n",
    "    for h in h_tags:\n",
    "        print(h,'\\n') # print header tags one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">Main Page</h1> \n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2> \n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2> \n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2> \n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2> \n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfl-h2\"><span id=\"From_today.27s_featured_list\"></span><span class=\"mw-headline\" id=\"From_today's_featured_list\">From today's featured list</span></h2> \n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2> \n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2> \n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2> \n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2> \n",
      "\n",
      "<h2>Navigation menu</h2> \n",
      "\n",
      "<h3 id=\"p-personal-label\">\n",
      "<span>Personal tools</span>\n",
      "</h3> \n",
      "\n",
      "<h3 id=\"p-namespaces-label\">\n",
      "<span>Namespaces</span>\n",
      "</h3> \n",
      "\n",
      "<h3 id=\"p-variants-label\">\n",
      "<span>Variants</span>\n",
      "</h3> \n",
      "\n",
      "<h3 id=\"p-views-label\">\n",
      "<span>Views</span>\n",
      "</h3> \n",
      "\n",
      "<h3 id=\"p-cactions-label\">\n",
      "<span>More</span>\n",
      "</h3> \n",
      "\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3> \n",
      "\n",
      "<h3 id=\"p-navigation-label\">\n",
      "<span>Navigation</span>\n",
      "</h3> \n",
      "\n",
      "<h3 id=\"p-interaction-label\">\n",
      "<span>Contribute</span>\n",
      "</h3> \n",
      "\n",
      "<h3 id=\"p-tb-label\">\n",
      "<span>Tools</span>\n",
      "</h3> \n",
      "\n",
      "<h3 id=\"p-coll-print_export-label\">\n",
      "<span>Print/export</span>\n",
      "</h3> \n",
      "\n",
      "<h3 id=\"p-wikibase-otherprojects-label\">\n",
      "<span>In other projects</span>\n",
      "</h3> \n",
      "\n",
      "<h3 id=\"p-lang-label\">\n",
      "<span>Languages</span>\n",
      "</h3> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the function to display all header tags, given an url\n",
    "all_header_tags('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that displays IMDb's top 100 movies data, given the url for IMDb's top charts webpage \n",
    "# and also saves the displayed data into csv file\n",
    "def imdb_top_100_movies(url) :\n",
    "    \n",
    "    page = requests.get(url) # request data from web page\n",
    "    soup = BeautifulSoup(page.content,'html.parser') # parsing \n",
    "    \n",
    "    names = soup.find_all('td',class_ = 'titleColumn') # find tags containing movie names\n",
    "    movie_names = [n.text for name in names for n in name.find_all('a')] # make list of movie names\n",
    "    \n",
    "    ratings = soup.find_all('td',class_ = 'ratingColumn imdbRating') # find tags containing ratings\n",
    "    IMDb_ratings = [r.text for rating in ratings for r in rating.find_all('strong')] # make list of ratings\n",
    "    \n",
    "    released = soup.find_all('span',class_ ='secondaryInfo') # find tags containing release year\n",
    "    release_years = [y.get_text().strip('()') for y in released] # make list of release years\n",
    "    \n",
    "    dict_ = {'Name':movie_names[0:100] ,'IMDB rating':IMDb_ratings[0:100] ,'Year of release':release_years[0:100]} #dictionary\n",
    "    df = pd.DataFrame(dict_) # create dataframe\n",
    "    \n",
    "    df.to_csv('IMDb_top_100_movies.csv') # save as csv file\n",
    "    \n",
    "    pd.set_option('display.max_rows', 100) # display 100 rows of df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>Year of release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>8.9</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Il buono, il brutto, il cattivo</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Inception</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Lord of the Rings: The Two Towers</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Star Wars: Episode V - The Empire Strikes Back</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Matrix</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>One Flew Over the Cuckoo's Nest</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Shichinin no samurai</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Se7en</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>La vita è bella</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Cidade de Deus</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Silence of the Lambs</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>It's a Wonderful Life</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Star Wars</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Saving Private Ryan</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The Green Mile</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sen to Chihiro no kamikakushi</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Interstellar</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Gisaengchung</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Léon</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Seppuku</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>The Usual Suspects</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>The Lion King</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>The Pianist</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Back to the Future</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Terminator 2: Judgment Day</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>American History X</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Modern Times</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Psycho</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Gladiator</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>The Departed</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>City Lights</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>The Intouchables</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Whiplash</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Hotaru no haka</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>The Prestige</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Once Upon a Time in the West</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Casablanca</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Nuovo Cinema Paradiso</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Rear Window</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Alien</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Apocalypse Now</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Memento</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>The Great Dictator</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Raiders of the Lost Ark</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Django Unchained</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>The Lives of Others</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Paths of Glory</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Joker</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>WALL·E</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>The Shining</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Avengers: Infinity War</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Sunset Blvd.</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Witness for the Prosecution</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Oldeuboi</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Mononoke-hime</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Spider-Man: Into the Spider-Verse</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Dr. Strangelove or: How I Learned to Stop Worr...</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Once Upon a Time in America</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Kimi no na wa.</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Aliens</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Coco</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>American Beauty</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Braveheart</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Capharnaüm</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Das Boot</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Tengoku to jigoku</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Amadeus</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Inglourious Basterds</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Star Wars: Episode VI - Return of the Jedi</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Good Will Hunting</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Reservoir Dogs</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2001: A Space Odyssey</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Requiem for a Dream</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Vertigo</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Jagten</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Eternal Sunshine of the Spotless Mind</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Dangal</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Full Metal Jacket</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Ladri di biciclette</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Singin' in the Rain</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name IMDB rating  \\\n",
       "0                            The Shawshank Redemption         9.2   \n",
       "1                                       The Godfather         9.1   \n",
       "2                              The Godfather: Part II         9.0   \n",
       "3                                     The Dark Knight         9.0   \n",
       "4                                        12 Angry Men         8.9   \n",
       "5                                    Schindler's List         8.9   \n",
       "6       The Lord of the Rings: The Return of the King         8.9   \n",
       "7                                        Pulp Fiction         8.8   \n",
       "8                     Il buono, il brutto, il cattivo         8.8   \n",
       "9   The Lord of the Rings: The Fellowship of the Ring         8.8   \n",
       "10                                         Fight Club         8.8   \n",
       "11                                       Forrest Gump         8.8   \n",
       "12                                          Inception         8.7   \n",
       "13              The Lord of the Rings: The Two Towers         8.7   \n",
       "14     Star Wars: Episode V - The Empire Strikes Back         8.7   \n",
       "15                                         The Matrix         8.6   \n",
       "16                                         Goodfellas         8.6   \n",
       "17                    One Flew Over the Cuckoo's Nest         8.6   \n",
       "18                               Shichinin no samurai         8.6   \n",
       "19                                              Se7en         8.6   \n",
       "20                                    La vita è bella         8.6   \n",
       "21                                     Cidade de Deus         8.6   \n",
       "22                           The Silence of the Lambs         8.6   \n",
       "23                              It's a Wonderful Life         8.6   \n",
       "24                                          Star Wars         8.6   \n",
       "25                                Saving Private Ryan         8.6   \n",
       "26                                     The Green Mile         8.5   \n",
       "27                      Sen to Chihiro no kamikakushi         8.5   \n",
       "28                                       Interstellar         8.5   \n",
       "29                                       Gisaengchung         8.5   \n",
       "30                                               Léon         8.5   \n",
       "31                                            Seppuku         8.5   \n",
       "32                                 The Usual Suspects         8.5   \n",
       "33                                      The Lion King         8.5   \n",
       "34                                        The Pianist         8.5   \n",
       "35                                 Back to the Future         8.5   \n",
       "36                         Terminator 2: Judgment Day         8.5   \n",
       "37                                 American History X         8.5   \n",
       "38                                       Modern Times         8.5   \n",
       "39                                             Psycho         8.5   \n",
       "40                                          Gladiator         8.5   \n",
       "41                                       The Departed         8.5   \n",
       "42                                        City Lights         8.5   \n",
       "43                                   The Intouchables         8.5   \n",
       "44                                           Whiplash         8.5   \n",
       "45                                     Hotaru no haka         8.5   \n",
       "46                                       The Prestige         8.5   \n",
       "47                       Once Upon a Time in the West         8.4   \n",
       "48                                         Casablanca         8.4   \n",
       "49                              Nuovo Cinema Paradiso         8.4   \n",
       "50                                        Rear Window         8.4   \n",
       "51                                              Alien         8.4   \n",
       "52                                     Apocalypse Now         8.4   \n",
       "53                                            Memento         8.4   \n",
       "54                                 The Great Dictator         8.4   \n",
       "55                            Raiders of the Lost Ark         8.4   \n",
       "56                                           Hamilton         8.4   \n",
       "57                                   Django Unchained         8.4   \n",
       "58                                The Lives of Others         8.4   \n",
       "59                                     Paths of Glory         8.4   \n",
       "60                                              Joker         8.4   \n",
       "61                                             WALL·E         8.4   \n",
       "62                                        The Shining         8.4   \n",
       "63                             Avengers: Infinity War         8.4   \n",
       "64                                       Sunset Blvd.         8.4   \n",
       "65                        Witness for the Prosecution         8.4   \n",
       "66                                           Oldeuboi         8.3   \n",
       "67                                      Mononoke-hime         8.3   \n",
       "68                  Spider-Man: Into the Spider-Verse         8.3   \n",
       "69  Dr. Strangelove or: How I Learned to Stop Worr...         8.3   \n",
       "70                              The Dark Knight Rises         8.3   \n",
       "71                        Once Upon a Time in America         8.3   \n",
       "72                                     Kimi no na wa.         8.3   \n",
       "73                                             Aliens         8.3   \n",
       "74                                               Coco         8.3   \n",
       "75                                  Avengers: Endgame         8.3   \n",
       "76                                    American Beauty         8.3   \n",
       "77                                         Braveheart         8.3   \n",
       "78                                         Capharnaüm         8.3   \n",
       "79                                           Das Boot         8.3   \n",
       "80                                           3 Idiots         8.3   \n",
       "81                                          Toy Story         8.3   \n",
       "82                                  Tengoku to jigoku         8.3   \n",
       "83                                            Amadeus         8.3   \n",
       "84                               Inglourious Basterds         8.3   \n",
       "85         Star Wars: Episode VI - Return of the Jedi         8.3   \n",
       "86                                  Good Will Hunting         8.3   \n",
       "87                                   Taare Zameen Par         8.3   \n",
       "88                                     Reservoir Dogs         8.3   \n",
       "89                              2001: A Space Odyssey         8.3   \n",
       "90                                Requiem for a Dream         8.3   \n",
       "91                                            Vertigo         8.3   \n",
       "92                  M - Eine Stadt sucht einen Mörder         8.3   \n",
       "93                                             Jagten         8.3   \n",
       "94              Eternal Sunshine of the Spotless Mind         8.3   \n",
       "95                                       Citizen Kane         8.3   \n",
       "96                                             Dangal         8.3   \n",
       "97                                  Full Metal Jacket         8.2   \n",
       "98                                Ladri di biciclette         8.2   \n",
       "99                                Singin' in the Rain         8.2   \n",
       "\n",
       "   Year of release  \n",
       "0             1994  \n",
       "1             1972  \n",
       "2             1974  \n",
       "3             2008  \n",
       "4             1957  \n",
       "5             1993  \n",
       "6             2003  \n",
       "7             1994  \n",
       "8             1966  \n",
       "9             2001  \n",
       "10            1999  \n",
       "11            1994  \n",
       "12            2010  \n",
       "13            2002  \n",
       "14            1980  \n",
       "15            1999  \n",
       "16            1990  \n",
       "17            1975  \n",
       "18            1954  \n",
       "19            1995  \n",
       "20            1997  \n",
       "21            2002  \n",
       "22            1991  \n",
       "23            1946  \n",
       "24            1977  \n",
       "25            1998  \n",
       "26            1999  \n",
       "27            2001  \n",
       "28            2014  \n",
       "29            2019  \n",
       "30            1994  \n",
       "31            1962  \n",
       "32            1995  \n",
       "33            1994  \n",
       "34            2002  \n",
       "35            1985  \n",
       "36            1991  \n",
       "37            1998  \n",
       "38            1936  \n",
       "39            1960  \n",
       "40            2000  \n",
       "41            2006  \n",
       "42            1931  \n",
       "43            2011  \n",
       "44            2014  \n",
       "45            1988  \n",
       "46            2006  \n",
       "47            1968  \n",
       "48            1942  \n",
       "49            1988  \n",
       "50            1954  \n",
       "51            1979  \n",
       "52            1979  \n",
       "53            2000  \n",
       "54            1940  \n",
       "55            1981  \n",
       "56            2020  \n",
       "57            2012  \n",
       "58            2006  \n",
       "59            1957  \n",
       "60            2019  \n",
       "61            2008  \n",
       "62            1980  \n",
       "63            2018  \n",
       "64            1950  \n",
       "65            1957  \n",
       "66            2003  \n",
       "67            1997  \n",
       "68            2018  \n",
       "69            1964  \n",
       "70            2012  \n",
       "71            1984  \n",
       "72            2016  \n",
       "73            1986  \n",
       "74            2017  \n",
       "75            2019  \n",
       "76            1999  \n",
       "77            1995  \n",
       "78            2018  \n",
       "79            1981  \n",
       "80            2009  \n",
       "81            1995  \n",
       "82            1963  \n",
       "83            1984  \n",
       "84            2009  \n",
       "85            1983  \n",
       "86            1997  \n",
       "87            2007  \n",
       "88            1992  \n",
       "89            1968  \n",
       "90            2000  \n",
       "91            1958  \n",
       "92            1931  \n",
       "93            2012  \n",
       "94            2004  \n",
       "95            1941  \n",
       "96            2016  \n",
       "97            1987  \n",
       "98            1948  \n",
       "99            1952  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the function to display top 100 movies from IMDb's web page, given an url\n",
    "imdb_top_100_movies('https://www.imdb.com/chart/top/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code :\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that displays IMDb's top 100 Indian movies data, given the url for IMDb's top charts webpage\n",
    "# and also saves the displayed data into csv file\n",
    "def imdb_top_100_Indian_movies(url) :\n",
    "    \n",
    "    page = requests.get(url) # request data from web page\n",
    "    soup = BeautifulSoup(page.content,'html.parser') # parsing \n",
    "    \n",
    "    names = soup.find_all('td',class_ = 'titleColumn') # find tags containing movie names\n",
    "    movie_names = [n.text for name in names for n in name.find_all('a')] # make list of movie names\n",
    "    \n",
    "    ratings = soup.find_all('td',class_ = 'ratingColumn imdbRating') # find tags containing ratings\n",
    "    IMDb_ratings = [r.text for rating in ratings for r in rating.find_all('strong')] # make list of ratings\n",
    "    \n",
    "    released = soup.find_all('span',class_ ='secondaryInfo') # find tags containing release year\n",
    "    release_years = [y.get_text().strip('()') for y in released] # make list of release years\n",
    "    \n",
    "    dict_ = {'Name':movie_names[0:100] ,'IMDB rating':IMDb_ratings[0:100] ,'Year of release':release_years[0:100]} #dictionary\n",
    "    df = pd.DataFrame(dict_) # create dataframe\n",
    "    \n",
    "    df.to_csv('IMDb_top_100_movies.csv') # save as csv file\n",
    "    \n",
    "    pd.set_option('display.max_rows', 100) # display 100 rows of df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>Year of release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pather Panchali</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gol Maal</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ratsasan</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apur Sansar</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Natsamrat</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kireedam</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pariyerum Perumal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Black Friday</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Manichitrathazhu</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Thevar Magan</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>96</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Kumbalangi Nights</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Visaaranai</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Thalapathi</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Aparajito</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Anand</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Jaane Bhi Do Yaaro</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Asuran</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Pyaasa</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Guide</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Kannathil Muthamittal</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chupke Chupke</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Kaithi</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Jersey</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Vikram Vedha</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Thani Oruvan</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Vada Chennai</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Soorarai Pottru</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Khosla Ka Ghosla!</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Aruvi</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Peranbu</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Super Deluxe</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Agent Sai Srinivasa Athreya</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Tumbbad</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Andhadhun</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Mahanati</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Kaakkaa Muttai</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Premam</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Dhuruvangal Pathinaaru</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Satya</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Shahid</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Bangalore Days</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Mudhalvan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Soodhu Kavvum</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Anniyan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Gangs of Wasseypur</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Papanasam</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Jigarthanda</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Bhaag Milkha Bhaag</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Paan Singh Tomar</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Talvar</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Swades: We, the People</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Hera Pheri</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Sholay</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Nil Battey Sannata</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Pithamagan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Black</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Chak De! India</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Jo Jeeta Wohi Sikandar</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Sairat</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Mughal-E-Azam</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Charulata</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Ustad Hotel</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Queen</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Udaan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Article 15</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Zindagi Na Milegi Dobara</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>A Wednesday</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Dil Chahta Hai</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Munna Bhai M.B.B.S.</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Sarfarosh</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Andaz Apna Apna</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Masaan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>OMG: Oh My God!</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Roja</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Lagaan: Once Upon a Time in India</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Maheshinte Prathikaaram</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Theeran adhigaaram ondru</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Kahaani</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Alai Payuthey</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>PK</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Baasha</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Uri: The Surgical Strike</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Iqbal</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Pink</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Barfi!</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>The Legend of Bhagat Singh</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Bommarillu</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Lucia</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Bombay</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Maqbool</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Omkara</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Section 375</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Name IMDB rating Year of release\n",
       "0                     Pather Panchali         8.5            1955\n",
       "1                            Gol Maal         8.5            1979\n",
       "2                            Ratsasan         8.5            2018\n",
       "3                             Nayakan         8.5            1987\n",
       "4                          Anbe Sivam         8.5            2003\n",
       "5                         Apur Sansar         8.4            1959\n",
       "6                           Natsamrat         8.4            2016\n",
       "7                            Kireedam         8.4            1989\n",
       "8                   Pariyerum Perumal         8.4            2018\n",
       "9                        Black Friday         8.4            2004\n",
       "10                   Manichitrathazhu         8.4            1993\n",
       "11                       Thevar Magan         8.4            1992\n",
       "12                                 96         8.4            2018\n",
       "13                  Kumbalangi Nights         8.4            2019\n",
       "14                           3 Idiots         8.4            2009\n",
       "15                   Taare Zameen Par         8.3            2007\n",
       "16                         Visaaranai         8.3            2015\n",
       "17                         Thalapathi         8.3            1991\n",
       "18                          Aparajito         8.3            1956\n",
       "19                              Anand         8.3            1971\n",
       "20                 Jaane Bhi Do Yaaro         8.3            1983\n",
       "21                             Asuran         8.3            2019\n",
       "22                             Pyaasa         8.3            1957\n",
       "23                              Guide         8.3            1965\n",
       "24              Kannathil Muthamittal         8.2            2002\n",
       "25                      Chupke Chupke         8.2            1975\n",
       "26                             Kaithi         8.2            2019\n",
       "27                             Jersey         8.2            2019\n",
       "28                       Vikram Vedha         8.2            2017\n",
       "29                       Thani Oruvan         8.2            2015\n",
       "30                       Vada Chennai         8.2            2018\n",
       "31                    Soorarai Pottru         8.2            2020\n",
       "32                           Drishyam         8.2            2013\n",
       "33                  Khosla Ka Ghosla!         8.2            2006\n",
       "34                              Aruvi         8.2            2016\n",
       "35                            Peranbu         8.2            2018\n",
       "36                       Super Deluxe         8.2            2019\n",
       "37        Agent Sai Srinivasa Athreya         8.2            2019\n",
       "38                            Tumbbad         8.2            2018\n",
       "39                          Andhadhun         8.2            2018\n",
       "40                           Mahanati         8.2            2018\n",
       "41                     Kaakkaa Muttai         8.2            2014\n",
       "42                             Premam         8.2            2015\n",
       "43             Dhuruvangal Pathinaaru         8.2            2016\n",
       "44                              Satya         8.2            1998\n",
       "45                             Shahid         8.2            2012\n",
       "46                     Bangalore Days         8.2            2014\n",
       "47                          Mudhalvan         8.1            1999\n",
       "48                      Soodhu Kavvum         8.1            2013\n",
       "49                            Anniyan         8.1            2005\n",
       "50                 Gangs of Wasseypur         8.1            2012\n",
       "51                          Papanasam         8.1            2015\n",
       "52                        Jigarthanda         8.1            2014\n",
       "53                 Bhaag Milkha Bhaag         8.1            2013\n",
       "54                   Paan Singh Tomar         8.1            2012\n",
       "55                             Talvar         8.1            2015\n",
       "56             Swades: We, the People         8.1            2004\n",
       "57                         Hera Pheri         8.1            2000\n",
       "58                             Sholay         8.1            1975\n",
       "59                           Drishyam         8.1            2015\n",
       "60                 Nil Battey Sannata         8.1            2015\n",
       "61                         Pithamagan         8.1            2003\n",
       "62                              Black         8.1            2005\n",
       "63                     Chak De! India         8.1            2007\n",
       "64             Jo Jeeta Wohi Sikandar         8.1            1992\n",
       "65                             Sairat         8.1            2016\n",
       "66                      Mughal-E-Azam         8.1            1960\n",
       "67                          Charulata         8.1            1964\n",
       "68                    Rang De Basanti         8.1            2006\n",
       "69                        Ustad Hotel         8.1            2012\n",
       "70                              Queen         8.1            2013\n",
       "71                              Udaan         8.1            2010\n",
       "72                         Article 15         8.1            2019\n",
       "73           Zindagi Na Milegi Dobara         8.1            2011\n",
       "74                        A Wednesday         8.1            2008\n",
       "75                     Dil Chahta Hai         8.1            2001\n",
       "76                Munna Bhai M.B.B.S.         8.1            2003\n",
       "77                          Sarfarosh         8.1            1999\n",
       "78                    Andaz Apna Apna         8.1            1994\n",
       "79                             Masaan         8.1            2015\n",
       "80                    OMG: Oh My God!         8.1            2012\n",
       "81                               Roja         8.1            1992\n",
       "82  Lagaan: Once Upon a Time in India         8.1            2001\n",
       "83            Maheshinte Prathikaaram         8.1            2016\n",
       "84           Theeran adhigaaram ondru         8.1            2017\n",
       "85                            Kahaani         8.1            2012\n",
       "86                      Alai Payuthey         8.1            2000\n",
       "87                                 PK         8.1            2014\n",
       "88                             Baasha         8.1            1995\n",
       "89           Uri: The Surgical Strike         8.1            2018\n",
       "90                              Iqbal         8.1            2005\n",
       "91                               Pink         8.0            2016\n",
       "92                             Barfi!         8.0            2012\n",
       "93         The Legend of Bhagat Singh         8.0            2002\n",
       "94                         Bommarillu         8.0            2006\n",
       "95                              Lucia         8.0            2013\n",
       "96                             Bombay         8.0            1995\n",
       "97                            Maqbool         8.0            2003\n",
       "98                             Omkara         8.0            2006\n",
       "99                        Section 375         8.0            2019"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the function to display top 100 Indian movies from IMDb's web page, given an url\n",
    "imdb_top_100_Indian_movies('https://www.imdb.com/india/top-rated-indian-movies/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Write a python program to scrap book name, author name, genre and book review of any 5 books from ‘www.bookpage.com’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # to select random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that scraps and displays book name, author name, genre and book review of any random 5 books\n",
    "# given the url of webpage \n",
    "def any_5_books(url) :\n",
    "    \n",
    "    rint = random.randint(1, 1000) #pick a random page number out of 1000 pages having book informations\n",
    "    if rint == 1 :\n",
    "        page = requests.get((url + '/reviews')) # request data from 1st page \n",
    "        page_ = 'NOTE : To verify the information of the output, please visit :' + ' ' + url + '/reviews' + '\\n\\n'\n",
    "    else :\n",
    "        page = requests.get((url + '/reviews?page=' + str(rint))) # request data from rint'th page \n",
    "        page_ = 'NOTE : To verify the information of the output, please visit :' + ' ' + url + '/reviews?page=' + str(rint) + '\\n\\n'\n",
    "        \n",
    "    soup = BeautifulSoup(page.content,'html.parser') # parsing datafrom randomly selected web page\n",
    "    \n",
    "    names = soup.find_all('h4',class_ = \"italic\") # list of parent tags containing book names\n",
    "    book_names = [n.text for name in names for n in name.find_all('a')] # make list of book names\n",
    "    \n",
    "    A_names = soup.find_all('div',class_ = \"flex-article-content\") # list of parent tags containing author names\n",
    "    author_names = [a.text.strip('\\n') for auth in A_names for a in auth.find_all('p',class_=\"sans bold\")] #make list of authors\n",
    "    \n",
    "    genres = soup.find_all('p',class_ = \"genre-links hidden-phone\") # list of parent tags containing genres\n",
    "    genre = []\n",
    "    for gen in genres : # a loop to create a list of genres\n",
    "        g = ''\n",
    "        for i in gen.find_all('a') : \n",
    "            g = g + i.text + '/'\n",
    "        genre.append(g.strip('/'))\n",
    "        \n",
    "    reviews = soup.find_all('div',class_ = 'read-full') # list of parent tags containing links to full reviews\n",
    "    review_urls = [u.get('href',None) for re in reviews for u in re.find_all('a')] # list of links to full reviews\n",
    "    full_reviews = []\n",
    "    for r in review_urls : # loop to create a list of full reviews\n",
    "        rev_page = requests.get('https://bookpage.com' + r)\n",
    "        rev_soup = BeautifulSoup(rev_page.content,'html.parser')\n",
    "        read_reviews = rev_soup.find_all('div',class_ = 'article-body')\n",
    "        for p in read_reviews :\n",
    "            full_reviews.append(p.text)\n",
    "    \n",
    "    rindexs = set()\n",
    "    while len(rindexs) < 5 :   # loop to create a list of non repeated random integers from 0 to 9\n",
    "        rindexs.add(random.randint(0,len(genre)-1))\n",
    "    \n",
    "    if len(full_reviews) == len(genre):\n",
    "        print(page_)\n",
    "        for x,index in enumerate(rindexs,start=1) : #loop to display data of 5 randomly selected books from a randomly selected page\n",
    "            print(x,'.')\n",
    "            print('Book Name :',book_names[index])\n",
    "            print('Author Name :',author_names[index])\n",
    "            print('Genre :',genre[index])\n",
    "            print('Full Review :',full_reviews[index])\n",
    "    else :\n",
    "        \n",
    "        any_5_books(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE : To verify the information of the output, please visit : https://bookpage.com/reviews?page=162\n",
      "\n",
      "\n",
      "1 .\n",
      "Book Name : The Invisible Emperor\n",
      "Author Name : Mark Braude\n",
      "Genre : Nonfiction/Biography & Memoir/Biography\n",
      "Full Review : \n",
      "\n",
      "Though it may be best suited for initiates of Napoleona, The Invisible Emperor details the deceptively calm but ultimately catastrophic interlude in the 25-year military career of one of history’s most famous soldiers, Napoleon.\n",
      "Historian Mark Braude has re-created a detailed description of the emperor’s 10-month exile from France to the island of Elba. Napoleon’s new dominion lay only six miles off the Tuscan coast and 30 miles from Napoleon’s native Corsica, but to his captors, it seemed isolated enough for security and near enough for scrutiny. At the time, Elba was a French territory, which allowed for some familiarity of language and custom for Napoleon, and his guards even cultivated some loyalty toward the stocky little soldier, who whether for comfort or clever camaraderie preferred worn-out and undecorated old military garb as he wandered about the countryside.\n",
      "Napoleon’s primary jailor, Neil Campbell, was an injured British colonel assigned, rather ambiguously, to “accompany” Napoleon in his new estate, which was quite lavish for an abdicated general. Still titled “emperor,” albeit only of Elba, Napoleon threw parties, ordered extensive renovations for his wife and son when they joined him in exile (they never did), had ships, horses and carriages delivered, and picked out several residences with good views. A man who once controlled nearly the entire European continent between Russian and Great Britain could now truly say he was lord of all he surveyed—86 square miles.\n",
      "But if Napoleon’s weakness was stubborn ambition, his strength was strategy. Within the year, with only a thousand or so soldiers, he marched to Paris without even a skirmish. The next few months, now known as “the Hundred Days,” culminated in the Battle of Waterloo and the deaths of nearly 50,00 combatants.\n",
      "Braude’s narrow focus on this “invisible” interlude dangles bits of psychological suppositions not always entirely supported, but his view of a man still caught up in his own self-image—one which, it must be admitted, was shared by many others—is intriguing.\n",
      "\n",
      "\n",
      "2 .\n",
      "Book Name : What If It’s Us\n",
      "Author Name : Becky Albertalli, Adam Silvera\n",
      "Genre : YA/YA Fiction/Romance\n",
      "Full Review : \n",
      "\n",
      "Arthur is only visiting New York for the summer, but a trip to the post office brings the teen face-to-face with a dreamy, box-carrying young man; they flirt but then quickly lose sight of each other during a flash mob. Arthur is crushing on “box boy,” but will he ever see him again? With only a crumpled shipping label as a clue, Arthur begins his search, and through social media sleuthing and a missed connection poster, he finally finds Ben. Their attraction is mutual, but lots of forces are conspiring against them, and they wonder if they are meant to be together (albeit temporarily) or if the universe is trying to send them a bigger message.\n",
      "Becky Albertalli (Simon vs. the Homo Sapiens Agenda) and Adam Silvera (More Happy Than Not) are stars of young adult fiction thanks to their authentic depictions of gay characters, and this collaboration will certainly boost their popularity. This not-to-miss addition to the YA canon seems tailor-made for a movie adaptation.\n",
      " \n",
      "This article was originally published in the October 2018 issue of BookPage. Download the entire issue for the Kindle or Nook.\n",
      "\n",
      "\n",
      "3 .\n",
      "Book Name : 99 Ways to Die\n",
      "Author Name : Ed Lin\n",
      "Genre : Mystery & Suspense/Mystery\n",
      "Full Review : \n",
      "\n",
      "Readers don’t have to wait long—not even to the end of page one—to get to the setup for Ed Lin’s latest Taipei Night Market mystery, 99 Ways to Die. There has been an abduction of a prominent businessman, who happens to be the father of protagonist Chen Jing-nan’s erstwhile classmate Peggy Lee (not the husky-voiced jazz singer Peggy Lee of “Fever” fame, but rather the youngest daughter in a family of Taiwanese aristocrats). The kidnappers’ ransom demands are not for money; instead, they want access to a computer chip, which Peggy Lee claims to know nothing about. But chances are good that Peggy Lee is playing for time and saving face in a society where face is everything. Jing-nan, for his part, is not someone you’d think of as a PI—he runs a popular food shop in a Taipei night market—but Peggy Lee is headstrong, and if she wants Jing-nan on the case, he has little choice but to assent. 99 Ways to Die is the third in the series and is the most fleshed out of the three. Ultimately, Lin’s books are most appealing for the insider’s look at Taiwanese culture, the motley crew of supporting cast and the multiple laughs per page.\n",
      " \n",
      "This article was originally published in the October 2018 issue of BookPage. Download the entire issue for the Kindle or Nook.\n",
      "\n",
      "\n",
      "4 .\n",
      "Book Name : November Road\n",
      "Author Name : Lou Berney\n",
      "Genre : Fiction/Historical Fiction\n",
      "Full Review : \n",
      "\n",
      "Novels revolving around the assassination of John F. Kennedy have become a genre unto themselves. There are plenty, and likely even more conspiracy theories to boot. So at first take, November Road, the new thriller from author Lou Berney, may seem like just another book to add to the stack. Berney, though, is not just another author. Through gorgeous prose, the Edgar, Macavity and Anthony Award-winning author of The Long and Faraway Gone elevates an otherwise simple cat-and-mouse story into a heartfelt journey of hope and discovery for two characters running from their pasts.\n",
      "While the loss of the president is certainly felt throughout November Road, it only serves as a backdrop to what is essentially a story of redemption. The novel follows Frank Guidry, an enforcer for mobster Carlos Marcello, whose hands are all over JFK’s death. Frank is tasked with retrieving and disposing of a getaway car parked near the scene of the crime in Dallas, and a hit man has been tasked with disposing of Frank once the job is done. Aware that his life is in jeopardy, Frank makes a desperate dash for freedom along Route 66.\n",
      "At the same time, young mother Charlotte Roy, along with her two daughters, is making her escape from a failed marriage in Oklahoma. Naturally, the storylines eventually cross as Frank encounters Charlotte, whom he sees as a way to throw off his pursuer. What begins as a convenient way to cover his tracks evolves into a serious romance between the two characters. But with a killer after Frank, the suspense builds toward a fateful showdown.\n",
      "In the end, November Road is more than the sum of its parts—a thrilling plot, an iconic period piece and unforgettable characters. Above all, it’s an American novel not to be missed.\n",
      " \n",
      "ALSO IN BOOKPAGE: Read our Q&A with Lou Berney for November Road.\n",
      "This article was originally published in the October 2018 issue of BookPage. Download the entire issue for the Kindle or Nook.\n",
      "\n",
      "\n",
      "5 .\n",
      "Book Name : Wrecked\n",
      "Author Name : Joe Ide\n",
      "Genre : Mystery & Suspense/Mystery\n",
      "Full Review : \n",
      "\n",
      "You know it’s going to be a bad day when you wake up amid a team of disgraced Abu Ghraib prison guards who have kidnapped you and are becoming significantly fed up with your unwillingness to answer their questions. The victim is Isaiah Quintabe, known in his California neighborhood by his initials, IQ. Wrecked is Joe Ide’s third novel featuring IQ, and it’s the first time IQ has a chance of expanding his business into a full-fledged private investigation agency. At any given time, IQ fields a number of cases, but the one that becomes central to Wrecked has to do with the machinations of a Blackwater- esque mercenary, a man with little in the way of scruples and lots in the way of sadistic behavior. Wrecked takes Ide’s unlikely hero into new territory, with foes that test his mettle in ways his previous adversaries could not even fathom, and with a possible love interest that exposes an entirely new facet of IQ’s character.\n",
      " \n",
      "This article was originally published in the October 2018 issue of BookPage. Download the entire issue for the Kindle or Nook.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the function to display data of any random books from bookpage's web page, given url\n",
    "any_5_books('https://bookpage.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. \n",
    "\n",
    "You have to scrape:\n",
    "\n",
    "i) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "ii) Top 10 ODI Batsmen in men along with the records of their team and rating.\n",
    "\n",
    "iii) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that displays Top 10 ODI cricket rankings men\n",
    "# given the url of webpage \n",
    "def top_10_ODI_cricket_teams_men(url) :\n",
    "    \n",
    "# i) code : for Top 10 ODI teams in men’s cricket along with the records for matches, points and rating\n",
    "    \n",
    "    page = requests.get((url + '/rankings/mens/team-rankings/odi')) # request data from web page\n",
    "    soup = BeautifulSoup(page.content,'html.parser') # parsing \n",
    "    \n",
    "    names = soup.find_all('span',class_=\"u-hide-phablet\") # find tags containing team names\n",
    "    team_names = [n.text for n in names] # make list of team names\n",
    "    \n",
    "    for rat in soup.find_all('td',class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "        rating_1 = rat.text.strip() # rating of rank 1 team\n",
    "    team_ratings = [r.text for r in soup.find_all('td',class_=\"table-body__cell u-text-right rating\")] # make list of ratings\n",
    "    team_ratings.insert(0,rating_1) # insert the rating of 1st team at the beginning of list\n",
    "    \n",
    "    for match in soup.find_all('td',class_=\"rankings-block__banner--matches\"):\n",
    "        match_1 = match.text # total matches of rank 1 team\n",
    "    for points in soup.find_all('td',class_=\"rankings-block__banner--points\"):\n",
    "        points_1 = points.text # points of rank 1 team\n",
    "    \n",
    "    center_texts = [c.text for c in soup.find_all('td',class_=\"table-body__cell u-center-text\")] # make list of all centre texts\n",
    "    \n",
    "    total_matches = []\n",
    "    for m in range(0,len(center_texts),2) : # loop to create list of total matches\n",
    "        total_matches.append(center_texts[m]) # all even indexes in list of center texts gives total matches\n",
    "    total_matches.insert(0,match_1) # insert the total matches of 1st team at the beginning of list\n",
    "    \n",
    "    points = []\n",
    "    for p in range(1,len(center_texts),2) : # loop to create list of points\n",
    "        points.append(center_texts[p]) # all odd indexes in list of center texts gives points\n",
    "    points.insert(0,points_1) # insert the points of 1st team at the beginning of list\n",
    "    \n",
    "    dict_ = {'Rank':list(range(1,11)),'Team':team_names[0:10],'Matches':total_matches[0:10],'Points':points[0:10],'Rating':team_ratings[0:10]} #dictionary\n",
    "    df = pd.DataFrame(dict_) # create dataframe\n",
    "    df.set_index('Rank',inplace=True)\n",
    "    \n",
    "    pd.set_option('display.max_rows', 10) # display 10 rows of df\n",
    "    \n",
    "    print('i) Data of Top 10 ODI cricket teams Men :\\n')\n",
    "    print(df,'\\n')\n",
    "    \n",
    "# ii) code : Top 10 ODI Batsmen in men along with the records of their team and rating\n",
    "\n",
    "    page_1 = requests.get((url + '/rankings/mens/player-rankings/odi/batting')) # request data from web page\n",
    "    soup_1 = BeautifulSoup(page_1.content,'html.parser') # parsing\n",
    "    \n",
    "    for bat in soup_1.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "        batsman_1 = bat.text.strip('\\n') # batsman of rank 1 \n",
    "    batsmen_names = [b.text.strip('\\n') for b in soup_1.find_all('td',class_=\"table-body__cell rankings-table__name name\")] #make list of batsmen\n",
    "    batsmen_names.insert(0,batsman_1) # insert the 1st batsman at the beginning of list\n",
    "    \n",
    "    for team in soup_1.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "        team_1 = team.text.strip() # batsman's team of rank 1 \n",
    "    batsmen_teams = [t.text for t in soup_1.find_all('span',class_=\"table-body__logo-text\")] # make list of batsmens' teams\n",
    "    batsmen_teams.insert(0,team_1) # insert the team of 1st batsman at the beginning of list\n",
    "    \n",
    "    for ratt in soup_1.find_all('div',class_=\"rankings-block__banner--rating\"):\n",
    "        ratt_1 = ratt.text      # batsman's rating of rank 1 \n",
    "    batsmen_rating = [rt.text for rt in soup_1.find_all('td',class_=\"table-body__cell rating\")] # make list of batsmen rating\n",
    "    batsmen_rating.insert(0,ratt_1) # insert the rating of 1st batsman at the beginning of list\n",
    "    \n",
    "    dict_1 = {'Rank':list(range(1,11)),'Batsman':batsmen_names[0:10],'Team':batsmen_teams[0:10],'Rating':batsmen_rating[0:10]} #dictionary\n",
    "    df_1 = pd.DataFrame(dict_1) # create dataframe\n",
    "    df_1.set_index('Rank',inplace=True)\n",
    "    \n",
    "    print('ii) Data of Top 10 ODI Batsmen :\\n')\n",
    "    print(df_1,'\\n')\n",
    "    \n",
    "# iii) code : Top 10 ODI bowlers along with the records of their team and rating\n",
    "\n",
    "    page_2 = requests.get((url + '/rankings/mens/player-rankings/odi/bowling')) # request data from web page\n",
    "    soup_2 = BeautifulSoup(page_2.content,'html.parser') # parsing\n",
    "    \n",
    "    for bow in soup_2.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "        bowler_1 = bow.text.strip('\\n') # bowler of rank 1 \n",
    "    bowler_names = [bo.text.strip('\\n') for bo in soup_2.find_all('td',class_=\"table-body__cell rankings-table__name name\")] #make list of bowlers\n",
    "    bowler_names.insert(0,bowler_1) # insert the 1st bowler at the beginning of list\n",
    "    \n",
    "    for team_bo in soup_2.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "        team_bo_1 = team_bo.text.strip() # bowler's team of rank 1 \n",
    "    bowler_teams = [tb.text for tb in soup_2.find_all('span',class_=\"table-body__logo-text\")] # make list of bowlers' teams\n",
    "    bowler_teams.insert(0,team_bo_1) # insert the team of 1st bowler at the beginning of list\n",
    "    \n",
    "    for bo_rat in soup_2.find_all('div',class_=\"rankings-block__banner--rating\"):\n",
    "        bo_rat_1 = bo_rat.text      # bowler's rating of rank 1 \n",
    "    bowler_rating = [rb.text for rb in soup_2.find_all('td',class_=\"table-body__cell rating\")] # make list of bowler rating\n",
    "    bowler_rating.insert(0,bo_rat_1) # insert the rating of 1st bowler at the beginning of list\n",
    "    \n",
    "    dict_2 = {'Rank':list(range(1,11)),'Bowler':bowler_names[0:10],'Team':bowler_teams[0:10],'Rating':bowler_rating[0:10]} #dictionary\n",
    "    df_2 = pd.DataFrame(dict_2) # create dataframe\n",
    "    df_2.set_index('Rank',inplace=True)\n",
    "    \n",
    "    print('iii) Data of Top 10 ODI Bowlers :\\n')\n",
    "    print(df_2,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i) Data of Top 10 ODI cricket teams Men :\n",
      "\n",
      "              Team Matches Points Rating\n",
      "Rank                                    \n",
      "1          England      44  5,405    123\n",
      "2            India      52  6,102    117\n",
      "3      New Zealand      32  3,716    116\n",
      "4        Australia      39  4,344    111\n",
      "5     South Africa      31  3,345    108\n",
      "6         Pakistan      35  3,490    100\n",
      "7       Bangladesh      37  3,366     91\n",
      "8        Sri Lanka      39  3,297     85\n",
      "9      West Indies      46  3,402     74\n",
      "10     Afghanistan      31  1,844     59 \n",
      "\n",
      "ii) Data of Top 10 ODI Batsmen :\n",
      "\n",
      "                  Batsman Team Rating\n",
      "Rank                                 \n",
      "1             Virat Kohli  IND    870\n",
      "2            Rohit Sharma  IND    842\n",
      "3              Babar Azam  PAK    837\n",
      "4             Ross Taylor   NZ    818\n",
      "5             Aaron Finch  AUS    791\n",
      "6     Francois du Plessis   SA    790\n",
      "7            David Warner  AUS    773\n",
      "8         Kane Williamson   NZ    765\n",
      "9         Quinton de Kock   SA    755\n",
      "10         Jonny Bairstow  ENG    754 \n",
      "\n",
      "iii) Data of Top 10 ODI Bowlers :\n",
      "\n",
      "                 Bowler Team Rating\n",
      "Rank                               \n",
      "1           Trent Boult   NZ    722\n",
      "2      Mujeeb Ur Rahman  AFG    708\n",
      "3        Jasprit Bumrah  IND    700\n",
      "4          Mehedi Hasan  BAN    694\n",
      "5          Chris Woakes  ENG    675\n",
      "6         Kagiso Rabada   SA    665\n",
      "7        Josh Hazlewood  AUS    660\n",
      "8     Mustafizur Rahman  BAN    658\n",
      "9         Mohammad Amir  PAK    647\n",
      "10          Pat Cummins  AUS    646 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the function to display cricket rankings from ‘www.icc-cricket.com’ web page, given an url\n",
    "top_10_ODI_cricket_teams_men('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’.\n",
    "\n",
    "You have to scrape:\n",
    "\n",
    "i) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "ii) Top 10 women’s ODI players along with the records of their team and rating.\n",
    "\n",
    "iii) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that displays Top 10 ODI cricket rankings women\n",
    "# given the url of webpage \n",
    "def top_10_ODI_cricket_teams_women(url) :\n",
    "    \n",
    "# i) code : for Top 10 ODI teams in women’s cricket along with the records for matches, points and rating\n",
    "    \n",
    "    page = requests.get((url + '/rankings/womens/team-rankings/odi')) # request data from web page\n",
    "    soup = BeautifulSoup(page.content,'html.parser') # parsing \n",
    "    \n",
    "    names = soup.find_all('span',class_=\"u-hide-phablet\") # find tags containing team names\n",
    "    team_names = [n.text for n in names] # make list of team names\n",
    "    \n",
    "    for rat in soup.find_all('td',class_=\"rankings-block__banner--rating u-text-right\"):\n",
    "        rating_1 = rat.text.strip() # rating of rank 1 team\n",
    "    team_ratings = [r.text for r in soup.find_all('td',class_=\"table-body__cell u-text-right rating\")] # make list of ratings\n",
    "    team_ratings.insert(0,rating_1) # insert the rating of 1st team at the beginning of list\n",
    "    \n",
    "    for match in soup.find_all('td',class_=\"rankings-block__banner--matches\"):\n",
    "        match_1 = match.text # total matches of rank 1 team\n",
    "    for points in soup.find_all('td',class_=\"rankings-block__banner--points\"):\n",
    "        points_1 = points.text # points of rank 1 team\n",
    "    \n",
    "    center_texts = [c.text for c in soup.find_all('td',class_=\"table-body__cell u-center-text\")] # make list of all centre texts\n",
    "    \n",
    "    total_matches = []\n",
    "    for m in range(0,len(center_texts),2) : # loop to create list of total matches\n",
    "        total_matches.append(center_texts[m]) # all even indexes in list of center texts gives total matches\n",
    "    total_matches.insert(0,match_1) # insert the total matches of 1st team at the beginning of list\n",
    "    \n",
    "    points = []\n",
    "    for p in range(1,len(center_texts),2) : # loop to create list of points\n",
    "        points.append(center_texts[p]) # all odd indexes in list of center texts gives points\n",
    "    points.insert(0,points_1) # insert the points of 1st team at the beginning of list\n",
    "    \n",
    "    dict_ = {'Rank':list(range(1,11)),'Team':team_names[0:10],'Matches':total_matches[0:10],'Points':points[0:10],'Rating':team_ratings[0:10]} #dictionary\n",
    "    df = pd.DataFrame(dict_)# create dataframe\n",
    "    df.set_index('Rank',inplace=True)\n",
    "    \n",
    "    pd.set_option('display.max_rows', 10) # display 10 rows of df\n",
    "    \n",
    "    print('i) Data of Top 10 ODI cricket teams Women :\\n')\n",
    "    print(df,'\\n')\n",
    "    \n",
    "# ii) code : Top 10 ODI players women along with the records of their team and rating\n",
    "\n",
    "    page_1 = requests.get((url + '/rankings/womens/player-rankings/odi/batting')) # request data from web page\n",
    "    soup_1 = BeautifulSoup(page_1.content,'html.parser') # parsing\n",
    "    \n",
    "    for bat in soup_1.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "        batsman_1 = bat.text.strip('\\n') # batsman of rank 1 \n",
    "    batsmen_names = [b.text.strip('\\n') for b in soup_1.find_all('td',class_=\"table-body__cell rankings-table__name name\")] #make list of batsmen\n",
    "    batsmen_names.insert(0,batsman_1) # insert the 1st batsman at the beginning of list\n",
    "    \n",
    "    for team in soup_1.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "        team_1 = team.text.strip() # batsman's team of rank 1 \n",
    "    batsmen_teams = [t.text for t in soup_1.find_all('span',class_=\"table-body__logo-text\")] # make list of batsmens' teams\n",
    "    batsmen_teams.insert(0,team_1) # insert the team of 1st batsman at the beginning of list\n",
    "    \n",
    "    for ratt in soup_1.find_all('div',class_=\"rankings-block__banner--rating\"):\n",
    "        ratt_1 = ratt.text      # batsman's rating of rank 1 \n",
    "    batsmen_rating = [rt.text for rt in soup_1.find_all('td',class_=\"table-body__cell rating\")] # make list of batsmen rating\n",
    "    batsmen_rating.insert(0,ratt_1) # insert the rating of 1st batsman at the beginning of list\n",
    "    \n",
    "    dict_1 = {'Rank':list(range(1,11)),'Player':batsmen_names[0:10],'Team':batsmen_teams[0:10],'Rating':batsmen_rating[0:10]} #dictionary\n",
    "    df_1 = pd.DataFrame(dict_1) # create dataframe\n",
    "    df_1.set_index('Rank',inplace=True)\n",
    "    \n",
    "    print('ii) :')\n",
    "    print('Data of Top 10 ODI Batting Women :\\n')\n",
    "    print(df_1,'\\n')\n",
    "\n",
    "    page_2 = requests.get((url + '/rankings/womens/player-rankings/odi/bowling')) # request data from web page\n",
    "    soup_2 = BeautifulSoup(page_2.content,'html.parser') # parsing\n",
    "    \n",
    "    for bow in soup_2.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "        bowler_1 = bow.text.strip('\\n') # bowler of rank 1 \n",
    "    bowler_names = [bo.text.strip('\\n') for bo in soup_2.find_all('td',class_=\"table-body__cell rankings-table__name name\")] #make list of bowlers\n",
    "    bowler_names.insert(0,bowler_1) # insert the 1st bowler at the beginning of list\n",
    "    \n",
    "    for team_bo in soup_2.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "        team_bo_1 = team_bo.text.strip() # bowler's team of rank 1 \n",
    "    bowler_teams = [tb.text for tb in soup_2.find_all('span',class_=\"table-body__logo-text\")] # make list of bowlers' teams\n",
    "    bowler_teams.insert(0,team_bo_1) # insert the team of 1st bowler at the beginning of list\n",
    "    \n",
    "    for bo_rat in soup_2.find_all('div',class_=\"rankings-block__banner--rating\"):\n",
    "        bo_rat_1 = bo_rat.text      # bowler's rating of rank 1 \n",
    "    bowler_rating = [rb.text for rb in soup_2.find_all('td',class_=\"table-body__cell rating\")] # make list of bowler rating\n",
    "    bowler_rating.insert(0,bo_rat_1) # insert the rating of 1st bowler at the beginning of list\n",
    "    \n",
    "    dict_2 = {'Rank':list(range(1,11)),'Bowler':bowler_names[0:10],'Team':bowler_teams[0:10],'Rating':bowler_rating[0:10]} #dictionary\n",
    "    df_2 = pd.DataFrame(dict_2) # create dataframe\n",
    "    df_2.set_index('Rank',inplace=True)\n",
    "    \n",
    "    print('Data of Top 10 ODI Bowlers Women :\\n')\n",
    "    print(df_2,'\\n')\n",
    "    \n",
    "# iii) Top 10 women’s ODI all-rounder along with the records of their team and rating\n",
    "    \n",
    "    page_3 = requests.get((url + '/rankings/womens/player-rankings/odi/all-rounder')) # request data from web page\n",
    "    soup_3 = BeautifulSoup(page_3.content,'html.parser') # parsing\n",
    "    \n",
    "    for al in soup_3.find_all('div',class_=\"rankings-block__banner--name-large\"):\n",
    "        al_1 = al.text.strip('\\n') # All-Rounder of rank 1 \n",
    "    al_names = [a.text.strip('\\n') for a in soup_3.find_all('td',class_=\"table-body__cell rankings-table__name name\")] #make list of All-Rounder\n",
    "    al_names.insert(0,al_1) # insert the 1st All-Rounder at the beginning of list\n",
    "    \n",
    "    for team_all in soup_3.find_all('div',class_=\"rankings-block__banner--nationality\"):\n",
    "        team_all_1 = team_bo.text.strip() # All-Rounder's team of rank 1 \n",
    "    all_r_teams = [ar.text for ar in soup_3.find_all('span',class_=\"table-body__logo-text\")] # make list of bowlers' teams\n",
    "    all_r_teams.insert(0,team_all_1) # insert the team of 1st All-Rounder at the beginning of list\n",
    "    \n",
    "    for al_rat in soup_3.find_all('div',class_=\"rankings-block__banner--rating\"):\n",
    "        al_rat_1 = al_rat.text      # All-Rounder's rating of rank 1 \n",
    "    al_rating = [alr.text for alr in soup_3.find_all('td',class_=\"table-body__cell rating\")] # make list of All-Rounder rating\n",
    "    al_rating.insert(0,al_rat_1) # insert the rating of 1st All-Rounder at the beginning of list\n",
    "    \n",
    "    dict_3 = {'Rank':list(range(1,11)),'All-Rounder':al_names[0:10],'Team':all_r_teams[0:10],'Rating':al_rating[0:10]} #dictionary\n",
    "    df_3 = pd.DataFrame(dict_3) # create dataframe\n",
    "    df_3.set_index('Rank',inplace=True)\n",
    "    \n",
    "    print('iii) Data of Top 10 ODI All-Rounder Women :\\n')\n",
    "    print(df_3,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i) Data of Top 10 ODI cricket teams Women :\n",
      "\n",
      "              Team Matches Points Rating\n",
      "Rank                                    \n",
      "1        Australia      15  2,436    162\n",
      "2            India      15  1,812    121\n",
      "3          England      14  1,670    119\n",
      "4     South Africa      19  2,090    110\n",
      "5      New Zealand      15  1,384     92\n",
      "6      West Indies      12  1,025     85\n",
      "7         Pakistan      15  1,101     73\n",
      "8       Bangladesh       5    306     61\n",
      "9        Sri Lanka      11    519     47\n",
      "10         Ireland       2     25     13 \n",
      "\n",
      "ii) :\n",
      "Data of Top 10 ODI Batting Women :\n",
      "\n",
      "                 Player Team Rating\n",
      "Rank                               \n",
      "1           Meg Lanning  AUS    749\n",
      "2       Stafanie Taylor   WI    746\n",
      "3          Alyssa Healy  AUS    741\n",
      "4       Smriti Mandhana  IND    732\n",
      "5     Amy Satterthwaite   NZ    723\n",
      "6        Tammy Beaumont  ENG    716\n",
      "7       Laura Wolvaardt   SA    691\n",
      "8          Ellyse Perry  AUS    691\n",
      "9           Mithali Raj  IND    687\n",
      "10          Lizelle Lee   SA    681 \n",
      "\n",
      "Data of Top 10 ODI Bowlers Women :\n",
      "\n",
      "              Bowler Team Rating\n",
      "Rank                            \n",
      "1      Jess Jonassen  AUS    804\n",
      "2       Megan Schutt  AUS    735\n",
      "3     Marizanne Kapp   SA    711\n",
      "4     Shabnim Ismail   SA    708\n",
      "5     Jhulan Goswami  IND    691\n",
      "6       Poonam Yadav  IND    679\n",
      "7      Shikha Pandey  IND    675\n",
      "8       Ellyse Perry  AUS    666\n",
      "9     Anya Shrubsole  ENG    645\n",
      "10     Deepti Sharma  IND    639 \n",
      "\n",
      "iii) Data of Top 10 ODI All-Rounder Women :\n",
      "\n",
      "           All-Rounder Team Rating\n",
      "Rank                              \n",
      "1         Ellyse Perry  AUS    460\n",
      "2      Stafanie Taylor   WI    410\n",
      "3       Marizanne Kapp   SA    396\n",
      "4        Deepti Sharma  IND    359\n",
      "5        Jess Jonassen  AUS    301\n",
      "6     Dane van Niekerk   SA    297\n",
      "7        Sophie Devine   NZ    289\n",
      "8       Natalie Sciver  ENG    273\n",
      "9        Shikha Pandey  IND    250\n",
      "10     Katherine Brunt  ENG    232 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the function to display cricket rankings women from ‘www.icc-cricket.com’ web page, given an url\n",
    "top_10_ODI_cricket_teams_women('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Write a python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The scraped data should include Product Name, Price, Image URL and Average Rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Requests' library was scraping incomplete web elements, so I used urllib ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that scrapes details of all the mobile phones under Rs. 20,000 listed on Amazon.in\n",
    "# data includes Product Name, Price, Image URL and Average Rating\n",
    "# given the url of webpage , and the last page of mobile phones under Rs. 20,000\n",
    "def phones_under_20k(url,last_page) :   \n",
    "    \n",
    "    ph_stars = [] # create needed empty lists\n",
    "    ph_names = [] \n",
    "    ph_price = []\n",
    "    ph_imgs = [] \n",
    "    ph_stars_full_list = [] \n",
    "    ph_names_full_list = [] \n",
    "    ph_price_full_list = []\n",
    "    ph_imgs_full_list = []\n",
    "    \n",
    "# below are the keys of different types of headers and their values respectively \n",
    "# using these to make multiple fake requests from different web browsers, \n",
    "# the goal is to try tricking the website to think that the requests are not automated  \n",
    "    keys = [\"Accept\",\"Accept-Language\",\"Referer\",\"Sec-Fetch-Dest\",\n",
    "        \"Sec-Fetch-Site\",\"Sec-Fetch-User\",\"Upgrade-Insecure-Requests\"]\n",
    "\n",
    "    values = [\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "          \"en-GB,en;q=0.9,mr-IN;q=0.8,mr;q=0.7,hi-IN;q=0.6,hi;q=0.5,en-US;q=0.4\",\"https://www.scrapehero.com/\",\n",
    "          \"document\",\"cross-site\",\"?1\",\"1\"]\n",
    "    \n",
    "    random_page_nos = list(range(1,last_page))\n",
    "    random.shuffle(random_page_nos)  #create a set of randomly arranged page numbers to scrap data randomly\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for pg in random_page_nos: # loop to select one random page at a time, out of total pages with required data to scrape\n",
    "        \n",
    "        if pg == 1 :\n",
    "            page = Request((url + '/s?i=electronics&bbn=1389401031&rh=n%3A976419031%2Cn%3A1389401031%2Cn%3A1389432031%2Cp_36%3A-2000000&dc&qid=1612600046&rnid=1318502031&ref=sr_nr_p_36_1'))\n",
    "        else :\n",
    "            page = Request((url + '/s?i=electronics&bbn=1389401031&rh=n%3A976419031%2Cn%3A1389401031%2Cn%3A1389432031%2Cp_36%3A-2000000&dc&page=' + str(pg) + '&qid=1612600046&rnid=1318502031&ref=sr_nr_p_36_1'))\n",
    "               \n",
    "        ran_index = random.randint(0,len(keys)-1) # to randomly select a header\n",
    "        \n",
    "        page.add_header(keys[ran_index],values[ran_index]) # add randomly selected header before opening the url \n",
    "        \n",
    "        toggle_switch = 'ON'\n",
    "        \n",
    "        try :\n",
    "            \n",
    "            content = urlopen(page).read() \n",
    "            \n",
    "        except :\n",
    "            \n",
    "            print('Will have to skip data from page', pg ,', request blocked.')\n",
    "            \n",
    "            toggle_switch = 'OFF'\n",
    "        \n",
    "        time.sleep(random.randint(5,10)) # take a break for few seconds, go slow with scraping\n",
    "        \n",
    "        if toggle_switch == 'ON' :\n",
    "            \n",
    "            soup = BeautifulSoup(content,'html.parser') # parsing \n",
    "            \n",
    "            names = soup.find_all('h2',class_=\"a-size-mini a-spacing-none a-color-base s-line-clamp-2\")  # find tags containing phone names\n",
    "            prices = soup.find_all('span',class_=\"a-price-whole\") # find tags containing phone prices\n",
    "            images = soup.find_all('a',class_=\"a-link-normal s-no-outline\") # find tags containing ph. img url\n",
    "            stars = soup.find_all('div',class_=\"a-row a-size-small\") # find tags containing average stars\n",
    "            \n",
    "            for n in names:          # extractable full_list of phone names\n",
    "                ph_names_full_list.append(n.text.strip(' \\n')) \n",
    "\n",
    "        \n",
    "            for p in prices :      # extractable full_list of phone prices\n",
    "                ph_price_full_list.append(p.text)\n",
    "\n",
    "         \n",
    "            for img in images :\n",
    "                i = img.find('img').get('src')   # extractable full_list of phone image url\n",
    "                ph_imgs_full_list.append(i) \n",
    "\n",
    "        \n",
    "            for st in stars:\n",
    "                s = st.find('span',class_=\"a-icon-alt\").text   # make list of average stars\n",
    "                ph_stars_full_list.append(s.strip(' \\n'))\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            if (len(prices)==len(images)) and (len(images)==len(stars)) :\n",
    "# only append data where all the data is available, and none of them is missing               \n",
    "                for n in names:          # list of phone names\n",
    "                    ph_names.append(n.text.strip(' \\n')) \n",
    "\n",
    "        \n",
    "                for p in prices :      # list of phone prices\n",
    "                    ph_price.append(p.text)\n",
    "\n",
    "         \n",
    "                for img in images :\n",
    "                    i = img.find('img').get('src')   # list of phone image url\n",
    "                    ph_imgs.append(i) \n",
    "\n",
    "        \n",
    "                for st in stars:\n",
    "                    s = st.find('span',class_=\"a-icon-alt\").text   # make list of average stars\n",
    "                    ph_stars.append(s.strip(' \\n'))\n",
    "    \n",
    "    print('\\nTotal pages scraped : ',counter)\n",
    "    print('\\nTotal extractable product names : ',len(ph_names_full_list),'\\n','List of 10 samples of Extracted product names :\\n',ph_names_full_list)\n",
    "    print('\\nNote : The web elements containing the names of the products is missing in the soup, hence the empty list.')\n",
    "    print('\\nTotal extractable prices : ',len(ph_price_full_list),'\\n','List of 10 samples of Extracted prices :\\n',random.sample(ph_price_full_list,10))\n",
    "    print('\\nTotal extractable image urls : ',len(ph_imgs_full_list),'\\n','List of 10 samples of Extracted image urls :\\n',random.sample(ph_imgs_full_list,10))\n",
    "    print('\\nTotal extractable average stars : ',len(ph_stars_full_list),'\\n','List of 10 samples of Extracted average stars :\\n',random.sample(ph_stars_full_list,10))\n",
    "    \n",
    "    print('\\nAs these lists are not of same length, we cannot create a useful data file from it for training purposes')\n",
    "    print('\\nBelow is the data that has some meaning to it, because all the values in all columns are aligned')\n",
    "    \n",
    "    dict_ = {'Price':ph_price,'Image url':ph_imgs,'Average Rating':ph_stars} # dictionary\n",
    "    \n",
    "    df = pd.DataFrame(dict_) # dataframe\n",
    "\n",
    "    df.to_csv('phones_under_20k.csv') # to csv\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will have to skip data from page 6 , request blocked.\n",
      "Will have to skip data from page 45 , request blocked.\n",
      "Will have to skip data from page 64 , request blocked.\n",
      "Will have to skip data from page 78 , request blocked.\n",
      "Will have to skip data from page 166 , request blocked.\n",
      "Will have to skip data from page 55 , request blocked.\n",
      "Will have to skip data from page 41 , request blocked.\n",
      "Will have to skip data from page 112 , request blocked.\n",
      "Will have to skip data from page 194 , request blocked.\n",
      "Will have to skip data from page 169 , request blocked.\n",
      "Will have to skip data from page 122 , request blocked.\n",
      "Will have to skip data from page 8 , request blocked.\n",
      "Will have to skip data from page 100 , request blocked.\n",
      "Will have to skip data from page 49 , request blocked.\n",
      "Will have to skip data from page 18 , request blocked.\n",
      "Will have to skip data from page 212 , request blocked.\n",
      "Will have to skip data from page 117 , request blocked.\n",
      "Will have to skip data from page 187 , request blocked.\n",
      "Will have to skip data from page 201 , request blocked.\n",
      "Will have to skip data from page 146 , request blocked.\n",
      "Will have to skip data from page 19 , request blocked.\n",
      "Will have to skip data from page 109 , request blocked.\n",
      "Will have to skip data from page 105 , request blocked.\n",
      "Will have to skip data from page 209 , request blocked.\n",
      "Will have to skip data from page 3 , request blocked.\n",
      "Will have to skip data from page 14 , request blocked.\n",
      "Will have to skip data from page 215 , request blocked.\n",
      "Will have to skip data from page 110 , request blocked.\n",
      "Will have to skip data from page 75 , request blocked.\n",
      "Will have to skip data from page 124 , request blocked.\n",
      "Will have to skip data from page 163 , request blocked.\n",
      "Will have to skip data from page 153 , request blocked.\n",
      "Will have to skip data from page 102 , request blocked.\n",
      "Will have to skip data from page 119 , request blocked.\n",
      "Will have to skip data from page 214 , request blocked.\n",
      "Will have to skip data from page 77 , request blocked.\n",
      "Will have to skip data from page 178 , request blocked.\n",
      "Will have to skip data from page 9 , request blocked.\n",
      "Will have to skip data from page 27 , request blocked.\n",
      "Will have to skip data from page 121 , request blocked.\n",
      "Will have to skip data from page 167 , request blocked.\n",
      "Will have to skip data from page 81 , request blocked.\n",
      "Will have to skip data from page 142 , request blocked.\n",
      "Will have to skip data from page 56 , request blocked.\n",
      "Will have to skip data from page 143 , request blocked.\n",
      "Will have to skip data from page 54 , request blocked.\n",
      "Will have to skip data from page 185 , request blocked.\n",
      "Will have to skip data from page 148 , request blocked.\n",
      "Will have to skip data from page 28 , request blocked.\n",
      "Will have to skip data from page 26 , request blocked.\n",
      "Will have to skip data from page 134 , request blocked.\n",
      "Will have to skip data from page 125 , request blocked.\n",
      "Will have to skip data from page 179 , request blocked.\n",
      "Will have to skip data from page 35 , request blocked.\n",
      "Will have to skip data from page 149 , request blocked.\n",
      "Will have to skip data from page 132 , request blocked.\n",
      "Will have to skip data from page 7 , request blocked.\n",
      "Will have to skip data from page 150 , request blocked.\n",
      "Will have to skip data from page 123 , request blocked.\n",
      "Will have to skip data from page 67 , request blocked.\n",
      "Will have to skip data from page 138 , request blocked.\n",
      "Will have to skip data from page 48 , request blocked.\n",
      "Will have to skip data from page 133 , request blocked.\n",
      "Will have to skip data from page 195 , request blocked.\n",
      "Will have to skip data from page 137 , request blocked.\n",
      "Will have to skip data from page 140 , request blocked.\n",
      "Will have to skip data from page 91 , request blocked.\n",
      "Will have to skip data from page 174 , request blocked.\n",
      "Will have to skip data from page 200 , request blocked.\n",
      "Will have to skip data from page 218 , request blocked.\n",
      "Will have to skip data from page 37 , request blocked.\n",
      "Will have to skip data from page 113 , request blocked.\n",
      "Will have to skip data from page 183 , request blocked.\n",
      "Will have to skip data from page 171 , request blocked.\n",
      "Will have to skip data from page 73 , request blocked.\n",
      "Will have to skip data from page 190 , request blocked.\n",
      "Will have to skip data from page 57 , request blocked.\n",
      "Will have to skip data from page 32 , request blocked.\n",
      "Will have to skip data from page 199 , request blocked.\n",
      "Will have to skip data from page 168 , request blocked.\n",
      "Will have to skip data from page 4 , request blocked.\n",
      "Will have to skip data from page 141 , request blocked.\n",
      "Will have to skip data from page 42 , request blocked.\n",
      "Will have to skip data from page 103 , request blocked.\n",
      "Will have to skip data from page 207 , request blocked.\n",
      "Will have to skip data from page 217 , request blocked.\n",
      "Will have to skip data from page 13 , request blocked.\n",
      "Will have to skip data from page 44 , request blocked.\n",
      "Will have to skip data from page 69 , request blocked.\n",
      "Will have to skip data from page 126 , request blocked.\n",
      "Will have to skip data from page 216 , request blocked.\n",
      "Will have to skip data from page 52 , request blocked.\n",
      "Will have to skip data from page 181 , request blocked.\n",
      "Will have to skip data from page 158 , request blocked.\n",
      "Will have to skip data from page 50 , request blocked.\n",
      "Will have to skip data from page 203 , request blocked.\n",
      "Will have to skip data from page 139 , request blocked.\n",
      "Will have to skip data from page 68 , request blocked.\n",
      "Will have to skip data from page 22 , request blocked.\n",
      "Will have to skip data from page 38 , request blocked.\n",
      "Will have to skip data from page 72 , request blocked.\n",
      "Will have to skip data from page 160 , request blocked.\n",
      "Will have to skip data from page 177 , request blocked.\n",
      "Will have to skip data from page 5 , request blocked.\n",
      "Will have to skip data from page 145 , request blocked.\n",
      "Will have to skip data from page 92 , request blocked.\n",
      "Will have to skip data from page 79 , request blocked.\n",
      "Will have to skip data from page 96 , request blocked.\n",
      "Will have to skip data from page 104 , request blocked.\n",
      "Will have to skip data from page 162 , request blocked.\n",
      "Will have to skip data from page 29 , request blocked.\n",
      "Will have to skip data from page 206 , request blocked.\n",
      "Will have to skip data from page 20 , request blocked.\n",
      "Will have to skip data from page 21 , request blocked.\n",
      "Will have to skip data from page 33 , request blocked.\n",
      "Will have to skip data from page 66 , request blocked.\n",
      "Will have to skip data from page 11 , request blocked.\n",
      "Will have to skip data from page 118 , request blocked.\n",
      "Will have to skip data from page 2 , request blocked.\n",
      "Will have to skip data from page 36 , request blocked.\n",
      "Will have to skip data from page 93 , request blocked.\n",
      "Will have to skip data from page 31 , request blocked.\n",
      "Will have to skip data from page 62 , request blocked.\n",
      "Will have to skip data from page 165 , request blocked.\n",
      "Will have to skip data from page 157 , request blocked.\n",
      "Will have to skip data from page 152 , request blocked.\n",
      "Will have to skip data from page 204 , request blocked.\n",
      "Will have to skip data from page 180 , request blocked.\n",
      "Will have to skip data from page 189 , request blocked.\n",
      "Will have to skip data from page 89 , request blocked.\n",
      "Will have to skip data from page 188 , request blocked.\n",
      "Will have to skip data from page 61 , request blocked.\n",
      "Will have to skip data from page 182 , request blocked.\n",
      "Will have to skip data from page 193 , request blocked.\n",
      "Will have to skip data from page 120 , request blocked.\n",
      "Will have to skip data from page 30 , request blocked.\n",
      "Will have to skip data from page 192 , request blocked.\n",
      "Will have to skip data from page 135 , request blocked.\n",
      "Will have to skip data from page 43 , request blocked.\n",
      "Will have to skip data from page 202 , request blocked.\n",
      "Will have to skip data from page 99 , request blocked.\n",
      "Will have to skip data from page 58 , request blocked.\n",
      "Will have to skip data from page 196 , request blocked.\n",
      "Will have to skip data from page 94 , request blocked.\n",
      "Will have to skip data from page 210 , request blocked.\n",
      "Will have to skip data from page 127 , request blocked.\n",
      "Will have to skip data from page 186 , request blocked.\n",
      "Will have to skip data from page 173 , request blocked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will have to skip data from page 147 , request blocked.\n",
      "Will have to skip data from page 24 , request blocked.\n",
      "Will have to skip data from page 128 , request blocked.\n",
      "Will have to skip data from page 95 , request blocked.\n",
      "Will have to skip data from page 84 , request blocked.\n",
      "Will have to skip data from page 76 , request blocked.\n",
      "Will have to skip data from page 12 , request blocked.\n",
      "\n",
      "Total pages scraped :  63\n",
      "\n",
      "Total extractable product names :  0 \n",
      " List of 10 samples of Extracted product names :\n",
      " []\n",
      "\n",
      "Note : The web elements containing the names of the products is missing in the soup, hence the empty list.\n",
      "\n",
      "Total extractable prices :  1227 \n",
      " List of 10 samples of Extracted prices :\n",
      " ['620', '3,000', '2,199', '1,249', '499', '1,049', '13,499', '139', '249', '7,500']\n",
      "\n",
      "Total extractable image urls :  1512 \n",
      " List of 10 samples of Extracted image urls :\n",
      " ['https://m.media-amazon.com/images/I/51Igd1z07oL._AC_UL320_.jpg', 'https://m.media-amazon.com/images/I/41NwcWAZFaL._AC_UL320_.jpg', 'https://m.media-amazon.com/images/I/61D6yy83zoL._AC_UL320_.jpg', 'https://m.media-amazon.com/images/I/310c6XlBlGL._AC_UL320_.jpg', 'https://m.media-amazon.com/images/I/61fvV3ztyGL._AC_UL320_.jpg', 'https://m.media-amazon.com/images/I/61wADBqHfZL._AC_UL320_.jpg', 'https://m.media-amazon.com/images/I/31kzhkDW8nL._AC_UL320_.jpg', 'https://m.media-amazon.com/images/I/71gmh2kfKNL._AC_UL320_.jpg', 'https://m.media-amazon.com/images/I/21tcduY9TnL._AC_UL320_.jpg', 'https://m.media-amazon.com/images/I/41QXyZU9tNL._AC_UL320_.jpg']\n",
      "\n",
      "Total extractable average stars :  678 \n",
      " List of 10 samples of Extracted average stars :\n",
      " ['4.1 out of 5 stars', '3.7 out of 5 stars', '1.0 out of 5 stars', '4.0 out of 5 stars', '3.4 out of 5 stars', '1.0 out of 5 stars', '4.3 out of 5 stars', '3.5 out of 5 stars', '5.0 out of 5 stars', '3.6 out of 5 stars']\n",
      "\n",
      "As these lists are not of same length, we cannot create a useful data file from it for training purposes\n",
      "\n",
      "Below is the data that has some meaning to it, because all the values in all columns are aligned\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Image url</th>\n",
       "      <th>Average Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Price, Image url, Average Rating]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phones_under_20k('https://www.amazon.in',219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Write a python program to extract information about the local weather from the National Weather Service website of USA, https://www.weather.gov/ for the city, San Francisco. You need to extract data about 7 day extended forecast display for the city. The data should include period, short description, temperature and description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that displays information about the local weather from the National Weather Service website of USA,\n",
    "# for the city, San Francisco, given the url for the webpage\n",
    "\n",
    "def local_weather(url) :\n",
    "    \n",
    "    page = requests.get((url + 'MapClick.php?textField1=37.78&textField2=-122.42#.YCDjMOgzbIX')) # request data from web page\n",
    "    soup = BeautifulSoup(page.content,'html.parser') # parsing \n",
    "    \n",
    "    periods = [per.find('b').text.strip() for per in soup.find_all('div',class_=\"col-sm-2 forecast-label\")]\n",
    "    \n",
    "    details = [det.text.strip() for det in soup.find_all('div',class_=\"col-sm-10 forecast-text\")]\n",
    "    \n",
    "    short_desc = [short.strip(', ') for det in details for short in re.findall('^.+,',det) ]\n",
    "    \n",
    "    tempratures = [temp.strip() for det in details for temp in re.findall(', with a (.+?[0-9]+)',det)]\n",
    "    \n",
    "    extras = []\n",
    "    for det in details:\n",
    "        if re.search('[0-9]+. (.+)',det) :\n",
    "            for ex in re.findall('[0-9]+. (.+)',det):\n",
    "                extras.append(ex.strip('.'))\n",
    "        else :\n",
    "            extras.append('')\n",
    "    \n",
    "    short_description = [x +' '+ y for x, y in zip(short_desc, extras)]\n",
    "    \n",
    "    dict_ = {'Period':periods,'Short Description':short_description,'Temprature description (Fahrenheit)':tempratures} # dictionary\n",
    "    \n",
    "    df = pd.DataFrame(dict_)\n",
    "    \n",
    "    pd.set_option('display.max_rows', 15) # display 15 rows of df\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    print('7 day extended forecast display of San Francisco CA, USA :\\n')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 day extended forecast display of San Francisco CA, USA :\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Short Description</th>\n",
       "      <th>Temprature description (Fahrenheit)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overnight</td>\n",
       "      <td>Mostly cloudy West southwest wind 5 to 7 mph</td>\n",
       "      <td>low around 46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monday</td>\n",
       "      <td>Patchy fog between 7am and 9am.  Otherwise, mostly cloudy Calm wind becoming west 5 to 9 mph in the afternoon</td>\n",
       "      <td>high near 58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monday Night</td>\n",
       "      <td>Mostly cloudy West wind 7 to 9 mph</td>\n",
       "      <td>low around 49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Mostly cloudy West wind 7 to 10 mph</td>\n",
       "      <td>high near 58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuesday Night</td>\n",
       "      <td>Partly cloudy West wind 5 to 8 mph becoming calm  after midnight</td>\n",
       "      <td>low around 47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>high near 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wednesday Night</td>\n",
       "      <td>Partly cloudy</td>\n",
       "      <td>low around 48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>A chance of showers.  Mostly cloudy</td>\n",
       "      <td>high near 58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Thursday Night</td>\n",
       "      <td>A chance of showers.  Mostly cloudy</td>\n",
       "      <td>low around 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Friday</td>\n",
       "      <td>A chance of showers, mainly before 4pm.  Partly sunny</td>\n",
       "      <td>high near 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Friday Night</td>\n",
       "      <td>A chance of showers.  Mostly cloudy</td>\n",
       "      <td>low around 49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Saturday</td>\n",
       "      <td>A chance of rain.  Mostly cloudy</td>\n",
       "      <td>high near 57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Saturday Night</td>\n",
       "      <td>A chance of rain.  Mostly cloudy</td>\n",
       "      <td>low around 48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sunday</td>\n",
       "      <td>A chance of rain.  Partly sunny</td>\n",
       "      <td>high near 56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Period  \\\n",
       "0         Overnight   \n",
       "1            Monday   \n",
       "2      Monday Night   \n",
       "3           Tuesday   \n",
       "4     Tuesday Night   \n",
       "5         Wednesday   \n",
       "6   Wednesday Night   \n",
       "7          Thursday   \n",
       "8    Thursday Night   \n",
       "9            Friday   \n",
       "10     Friday Night   \n",
       "11         Saturday   \n",
       "12   Saturday Night   \n",
       "13           Sunday   \n",
       "\n",
       "                                                                                                Short Description  \\\n",
       "0                                                                    Mostly cloudy West southwest wind 5 to 7 mph   \n",
       "1   Patchy fog between 7am and 9am.  Otherwise, mostly cloudy Calm wind becoming west 5 to 9 mph in the afternoon   \n",
       "2                                                                              Mostly cloudy West wind 7 to 9 mph   \n",
       "3                                                                             Mostly cloudy West wind 7 to 10 mph   \n",
       "4                                                Partly cloudy West wind 5 to 8 mph becoming calm  after midnight   \n",
       "5                                                                                                          Sunny    \n",
       "6                                                                                                  Partly cloudy    \n",
       "7                                                                            A chance of showers.  Mostly cloudy    \n",
       "8                                                                            A chance of showers.  Mostly cloudy    \n",
       "9                                                          A chance of showers, mainly before 4pm.  Partly sunny    \n",
       "10                                                                           A chance of showers.  Mostly cloudy    \n",
       "11                                                                              A chance of rain.  Mostly cloudy    \n",
       "12                                                                              A chance of rain.  Mostly cloudy    \n",
       "13                                                                               A chance of rain.  Partly sunny    \n",
       "\n",
       "   Temprature description (Fahrenheit)  \n",
       "0                        low around 46  \n",
       "1                         high near 58  \n",
       "2                        low around 49  \n",
       "3                         high near 58  \n",
       "4                        low around 47  \n",
       "5                         high near 60  \n",
       "6                        low around 48  \n",
       "7                         high near 58  \n",
       "8                        low around 50  \n",
       "9                         high near 59  \n",
       "10                       low around 49  \n",
       "11                        high near 57  \n",
       "12                       low around 48  \n",
       "13                        high near 56  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_weather('https://forecast.weather.gov/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
